# ğŸ® Ready to Train!

Your Clash Royale RL system is **completely set up** and ready to train locally on your Mac.

## âœ… What's Been Implemented

### 1. Tower HP Detection âœ“
- OCR-based HP detection for all 6 towers
- Integrated into RL state encoding
- Auto-calibrated coordinates
- Cached updates (every 2 seconds) for performance

### 2. RL Training System âœ“
- **DQN Agent** with experience replay
- **Neural Network** (512â†’256â†’128 hidden layers)
- **Reward Calculator** using tower HP damage
- **State Encoder** with tower HP (1171-value state vector)
- **Auto-save checkpoints** every 5 games

### 3. Complete Integration âœ“
- Tower HP â†’ State â†’ Agent â†’ Action â†’ Reward â†’ Learning
- Trains while playing (online learning)
- Works on Mac CPU or Apple Silicon MPS
- Automatically resumes from checkpoints

## ğŸš€ Start Training NOW

```bash
cd /Users/eden/Desktop/clash-royale-rl

# Start training (100 games recommended)
python3 main.py --rl --games 100
```

### What You'll See

```
============================================================
Initializing RL Training System
============================================================
State size: 1171
  - Elixir: 1
  - Hand: 12
  - Enemy grid: 576
  - Ally grid: 576
  - Tower HP: 6
Using Apple Silicon MPS for training
============================================================
RL Training Enabled!
State size: 1171
Action size: 2304
============================================================

Agent is now playing 100 games...

[RL PLAY] knight at row 24, col 8 (epsilon: 0.950)
[RL PLAY] archers at row 28, col 14 (epsilon: 0.947)

[REWARD] Destroyed enemy_left_princess! Bonus: +10.0

[RL STATS] Steps: 100, Epsilon: 0.845, Avg Reward: 12.45, Avg Loss: 0.0234

[BATTLE RESULT] VICTORY

[RL EPISODE END]
  Episodes: 1
  Avg Reward (100): 45.30
  Epsilon: 0.820
  Buffer: 127/10000

Checkpoint saved: checkpoints/latest.pt
```

## ğŸ“Š Training Files

All created and ready:

```
rl/
â”œâ”€â”€ dqn_agent.py          âœ… DQN neural network + training
â”œâ”€â”€ reward_calculator.py  âœ… Reward system with tower HP
â””â”€â”€ state_encoder.py      âœ… State encoding (tower HP integrated)

detection/
â””â”€â”€ tower_hp_detector.py  âœ… OCR-based HP detection

main.py                   âœ… Updated with RL integration
checkpoints/              âœ… Auto-saves models here

Documentation:
â”œâ”€â”€ RL_TRAINING_GUIDE.md  ğŸ“– Complete training guide
â”œâ”€â”€ TOWER_HP_INTEGRATION.md ğŸ“– Tower HP integration details
â””â”€â”€ READY_TO_TRAIN.md     ğŸ“– This file
```

## ğŸ¯ Training Phases

### Phase 1: Exploration (Games 1-50)
- **Epsilon**: 1.0 â†’ 0.6 (mostly random)
- **Goal**: Collect diverse experiences
- **Expect**: Losses, learning basics

### Phase 2: Learning (Games 51-200)
- **Epsilon**: 0.6 â†’ 0.2 (mix of random + learned)
- **Goal**: Learn strategy
- **Expect**: Gradual improvement

### Phase 3: Mastery (Games 201+)
- **Epsilon**: 0.2 â†’ 0.1 (mostly learned policy)
- **Goal**: Optimize play
- **Expect**: Consistent performance

## ğŸ’¡ Quick Commands

```bash
# Basic training
python3 main.py --rl --games 100

# Long session (overnight)
python3 main.py --rl --games 500

# Resume training (automatic from latest.pt)
python3 main.py --rl --games 50

# Start fresh
rm -rf checkpoints/
python3 main.py --rl --games 100

# Test tower HP detection first
python3 scripts/test_tower_hp.py
```

## ğŸ”¬ How It Works

```
1. OBSERVE
   â”œâ”€â”€ Elixir (fast detection)
   â”œâ”€â”€ Hand (4 cards, YOLO classifier)
   â”œâ”€â”€ Tower HP (OCR, cached every 2s)
   â””â”€â”€ Troops (YOLO object detection)
         â†“
2. ENCODE STATE
   â””â”€â”€ 1171-value vector
         â†“
3. AGENT SELECTS ACTION
   â”œâ”€â”€ Exploration (random) OR
   â””â”€â”€ Exploitation (neural network)
         â†“
4. EXECUTE ACTION
   â””â”€â”€ Play card at (slot, row, col)
         â†“
5. CALCULATE REWARD
   â”œâ”€â”€ Tower damage: +0.01/HP
   â”œâ”€â”€ Tower destroyed: +10.0
   â”œâ”€â”€ Win: +100.0
   â””â”€â”€ Loss: -100.0
         â†“
6. LEARN
   â”œâ”€â”€ Store experience in replay buffer
   â”œâ”€â”€ Sample batch (64 transitions)
   â”œâ”€â”€ Train neural network
   â””â”€â”€ Update target network every 1000 steps
         â†“
7. REPEAT
```

## ğŸ® Reward System

| Event | Reward |
|-------|--------|
| Enemy tower damage | +0.01 per HP |
| Enemy tower destroyed | +10.0 |
| Ally tower damage | -0.01 per HP |
| Ally tower destroyed | -10.0 |
| Victory | +100.0 |
| Defeat | -100.0 |
| Ally troops on field | +0.05 each |
| Good elixir management | +0.1 |

## ğŸ“ˆ What to Expect

### First 10 Games
- Random, chaotic play
- Mostly losses
- Learning basics (where to place cards)

### Games 11-50
- Mix of random and strategic
- Occasional wins
- Learning counters and defense

### Games 51-100
- Increasingly strategic
- More wins
- Learning tower targeting

### Games 100+
- Consistent strategy
- Good win rate
- Optimizing placement and timing

## âš¡ Performance

- **Training Speed**: ~10ms per step (Apple Silicon) / ~30ms (Intel)
- **Game Duration**: ~3-5 minutes each
- **100 Games**: ~6-8 hours
- **Memory**: ~500MB agent + ~2GB YOLO models

## ğŸ› ï¸ Troubleshooting

### Agent not learning?
- Check YOLO is detecting troops: `python3 main.py --model --games 1`
- Let it train longer (100+ games minimum)
- Check epsilon is decreasing (shown in stats)

### Tower HP not detecting?
```bash
python3 scripts/test_tower_hp.py
```
Should show HP values for all towers in battle.

### Out of memory?
Force CPU mode:
```python
# In main.py, line ~88
device='cpu'
```

### Training too slow?
Normal! Each game takes 3-5 minutes. This is real-time learning.

## ğŸ“š Documentation

- **[RL_TRAINING_GUIDE.md](RL_TRAINING_GUIDE.md)** - Complete training guide
- **[TOWER_HP_INTEGRATION.md](TOWER_HP_INTEGRATION.md)** - Tower HP system details
- **[TOWER_HP_QUICKSTART.md](TOWER_HP_QUICKSTART.md)** - Quick reference

## ğŸ‰ You're All Set!

Everything is implemented and tested. Just run:

```bash
python3 main.py --rl --games 100
```

The agent will:
1. Start playing games in BlueStacks
2. Learn from experience in real-time
3. Save checkpoints every 5 games
4. Gradually improve over time

**Let it train overnight for best results!** ğŸŒ™

---

## ğŸ’­ Questions?

Check the guides:
- Training issues â†’ [RL_TRAINING_GUIDE.md](RL_TRAINING_GUIDE.md)
- Tower HP issues â†’ [TOWER_HP_INTEGRATION.md](TOWER_HP_INTEGRATION.md)
- Quick commands â†’ [TOWER_HP_QUICKSTART.md](TOWER_HP_QUICKSTART.md)

Good luck! ğŸš€
